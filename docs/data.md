# Data Preparation

After the organization, we expect the directory structure to be the following:

Notice: `SgMg` and `datasets` are in the same folder.

```
├── SgMg
├── datasets
│   ├── coco
│       ├── train2014
│       ├── refcoco
│           ├── instances_refcoco_train.json
│           ├── instances_refcoco_val.json
│       ├── refcoco+
│           ├── instances_refcoco+_train.json
│           ├── instances_refcoco+_val.json
│       ├── refcocog
│           ├── instances_refcocog_train.json
│           ├── instances_refcocog_val.json
│   ├── refer_youtube_vos
│         ├── meta_expressions
│         ├── train
│               ├── JPEGImages
│               ├── Annotations
│               ├── meta.json
│         ├── valid
│               ├── JPEGImages
│   ├── refer_davis
│         ├── meta_expressions
│         ├── valid
│               ├── JPEGImages
│                     ├── 480p
│               ├── Annotations
│               ├── ImageSets
│               ├── meta.json
│   ├── a2d_sentences
│       ├── Release
│       ├── text_annotations
│           ├── a2d_annotation_with_instances
│           ├── a2d_annotation.txt
│           ├── a2d_missed_videos.txt
│       ├── a2d_sentences_single_frame_test_annotations.json
│       ├── a2d_sentences_single_frame_train_annotations.json
│       ├── a2d_sentences_test_annotations_in_coco_format.json
│   ├── jhmdb_sentences
│       ├── Rename_Images
│       ├── puppet_mask
│       ├── jhmdb_annotation.txt
│       ├── jhmdb_sentences_samples_metadata.json
│       ├── jhmdb_sentences_gt_annotations_in_coco_format.json
...
```

## Ref-COCO

Download the dataset from the official website [COCO](https://cocodataset.org/#download).   
RefCOCO/+/g use the COCO2014 train split.
Download the annotation files from [github](https://github.com/lichengunc/refer).

Convert the annotation files:

```
python3 tools/data/convert_refexp_to_coco.py
```

Finally, we expect the directory structure to be the following:

```
├── datasets
│   ├── coco
│       ├── train2014
│       ├── refcoco
│           ├── instances_refcoco_train.json
│           ├── instances_refcoco_val.json
│       ├── refcoco+
│           ├── instances_refcoco+_train.json
│           ├── instances_refcoco+_val.json
│       ├── refcocog
│           ├── instances_refcocog_train.json
│           ├── instances_refcocog_val.json
```

## refer_youtube_vos

Download the dataset from the competition's website [here](https://competitions.codalab.org/competitions/29139#participate-get_data).
Then, extract and organize the file. We expect the directory structure to be the following:

```
├── datasets
│   ├── refer_youtube_vos
│         ├── meta_expressions
│         ├── train
│               ├── JPEGImages
│               ├── Annotations
│               ├── meta.json
│         ├── valid
│               ├── JPEGImages
```

## refer_davis17

**Notice: We recommend to directly download the parsed Ref-DAVIS dataset from the** [Google Drive](https://drive.google.com/file/d/1W0RsdxMK3VkNL80H1OWNmia-2asdCyYF/view?usp=sharing) **to avoid the following steps.**

Download the DAVIS2017 dataset from the [website](https://davischallenge.org/davis2017/code.html). Note that you only need to download the two zip files `DAVIS-2017-Unsupervised-trainval-480p.zip` and `DAVIS-2017_semantics-480p.zip`.
Download the text annotations from the [website](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/video-segmentation/video-object-segmentation-with-language-referring-expressions).
Then, put the zip files in the directory as follows.

```
├── datasets
│   ├── refer_davis
│   │   ├── DAVIS-2017_semantics-480p.zip
│   │   ├── DAVIS-2017-Unsupervised-trainval-480p.zip
│   │   ├── davis_text_annotations.zip
```

Unzip these zip files.
```
unzip -o davis_text_annotations.zip
unzip -o DAVIS-2017_semantics-480p.zip
unzip -o DAVIS-2017-Unsupervised-trainval-480p.zip
```

Preprocess the dataset to refer_youtube_vos format. (Make sure you are in the main directory)

```
python tools/data/convert_davis_to_ytvos.py
```

Finally, unzip the file `DAVIS-2017-Unsupervised-trainval-480p.zip` again (since we use `mv` in preprocess for efficiency).

```
unzip -o DAVIS-2017-Unsupervised-trainval-480p.zip
```

## A2D-Sentences

Follow the instructions and download the dataset from the website [here](https://kgavrilyuk.github.io/publication/actor_action/). 
Then, extract the files. Additionally, we use the same json annotation files generated by [MTTR](https://github.com/mttr2021/MTTR). Please download these files from [google drive](https://drive.google.com/drive/u/0/folders/1daTuACcZUKuzgl0iqzwCfKm_tSISarFl).
We expect the directory structure to be the following:

```
├── datasets
│   ├── a2d_sentences
│   │   ├── Release
│   │   ├── text_annotations
│   │   │   ├── a2d_annotation_with_instances
│   │   │   ├── a2d_annotation.txt
│   │   │   ├── a2d_missed_videos.txt
│   │   ├── a2d_sentences_single_frame_test_annotations.json
│   │   ├── a2d_sentences_single_frame_train_annotations.json
│   │   ├── a2d_sentences_test_annotations_in_coco_format.json
```

## JHMDB-Sentences

Follow the instructions and download the dataset from the website [here](https://kgavrilyuk.github.io/publication/actor_action/). 
Then, extract the files. Additionally, we use the same json annotation files generated by [MTTR](https://github.com/mttr2021/MTTR). Please download these files from [google drive](https://drive.google.com/drive/u/0/folders/1sXmjpWmc0GxYIz-EFLw5S9dJvmGJAPqx).
We expect the directory structure to be the following:

```
├── datasets
│   ├── jhmdb_sentences
│   │   ├── Rename_Images
│   │   ├── puppet_mask
│   │   ├── jhmdb_annotation.txt
│   │   ├── jhmdb_sentences_samples_metadata.json
│   │   ├── jhmdb_sentences_gt_annotations_in_coco_format.json
```
